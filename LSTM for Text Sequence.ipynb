{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python3.6\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "times_steps = 6\n",
    "element_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['six eight eight four six pad', 'eight two four four two pad', 'eight two eight six eight two', 'eight six four two four four', 'two four four six pad pad', 'two six two six pad pad'] nine five five one pad pad\n"
     ]
    }
   ],
   "source": [
    "# 生成数据\n",
    "digit_to_word_map = {1:'one', 2:'two', 3:'three', 4:'four', 5:'five', 6:'six',\n",
    "                    7:'seven', 8:'eight', 9:'nine'}\n",
    "digit_to_word_map[0] = 'pad'\n",
    "\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2), rand_seq_len)\n",
    "    \n",
    "    # Padding\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))\n",
    "    \n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "\n",
    "data = even_sentences+odd_sentences\n",
    "seqlens *= 2\n",
    "print(data[0:6], data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'six': 0, 'eight': 1, 'four': 2, 'pad': 3, 'two': 4, 'five': 5, 'three': 6, 'seven': 7, 'one': 8, 'nine': 9} {0: 'six', 1: 'eight', 2: 'four', 3: 'pad', 4: 'two', 5: 'five', 6: 'three', 7: 'seven', 8: 'one', 9: 'nine'}\n"
     ]
    }
   ],
   "source": [
    "# map words to indices, like a real NLP application dealing with real data\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "\n",
    "for sen in data:\n",
    "    for word in sen.split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "\n",
    "index2word_map = {index:word for word,index in word2index_map.items()}\n",
    "vocabulary_size = len(word2index_map)\n",
    "print(word2index_map, index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机数据集\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    tmp = [0]*2\n",
    "    tmp[label] = 1\n",
    "    labels[i] = tmp\n",
    "\n",
    "data_slices = list(range(len(data)))\n",
    "np.random.shuffle(data_slices)\n",
    "data = np.array(data)[data_slices]\n",
    "labels = np.array(labels)[data_slices]\n",
    "seqlens = np.array(seqlens)[data_slices]\n",
    "\n",
    "train_x = data[:10000] \n",
    "train_y = labels[:10000] \n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:] \n",
    "test_y = labels[10000:] \n",
    "test_seqlens = seqlens[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_batch(batch_size, data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    \n",
    "    x = [[word2index_map[word] for word in data_x[i].split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    \n",
    "    return x, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = tf.placeholder(tf.int32, shape=[batch_size, times_steps])\n",
    "_y = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "\n",
    "# seqlen for dynamic calculation\n",
    "_seqlen = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词嵌入向量(WordEmbedding处理\n",
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size,\n",
    "                          embedding_dimension],\n",
    "                         -1.0, 1.0), name='embedding')\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-50fbdc776a63>:3: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-9-50fbdc776a63>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('lstm'):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,\n",
    "                                            forget_bias=1.0)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed, sequence_length=_seqlen,\n",
    "                                       dtype=tf.float32)\n",
    "    # states: it conveniently stores for us the last relevant output vector—\n",
    "    # its values match the last relevant output vector before zero-padding\n",
    "    \n",
    "weights = {\n",
    "    'linear_layer':tf.Variable(tf.truncated_normal([hidden_layer_size,\n",
    "                                                   num_classes],\n",
    "                                                  mean=0, stddev=0.01))\n",
    "}\n",
    "biases = {\n",
    "    'linear_layer':tf.Variable(tf.truncated_normal([num_classes],\n",
    "                                                  mean=0, stddev=0.01))\n",
    "}\n",
    "\n",
    "final_output = tf.matmul(states[1],\n",
    "                        weights['linear_layer']) + biases['linear_layer']\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output, labels=_y)\n",
    "cross_entropy = tf.reduce_mean(softmax)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(final_output, 1), tf.argmax(_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 50.00000\n",
      "Accuracy at 100: 100.00000\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sentence_batch(batch_size, train_x, train_y, seqlens)\n",
    "        sess.run(train_step, feed_dict={_input:x_batch, _y:y_batch, _seqlen:seqlen_batch})\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={_input:x_batch, _y:y_batch, _seqlen:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (i, acc))\n",
    "        \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sentence_batch(batch_size, test_x, test_y, test_seqlens)\n",
    "        batch_pred, batch_acc = sess.run([tf.argmax(final_output, 1), accuracy], feed_dict={_input:x_test, _y:y_test, _seqlen:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "    \n",
    "    output_example = sess.run([outputs], feed_dict={_input:x_test, _y:y_test, _seqlen:seqlen_test}) # time_steps * hidden_layer_size\n",
    "    states_example = sess.run([states[1]], feed_dict={_input:x_test, _y:y_test, _seqlen:seqlen_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 6, 32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(output_example[0].shape) # 32: hidden_layer_size\n",
    "print(len(output_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(states_example[0].shape)\n",
    "print(len(states_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking multiple LSTMs \n",
    "num_LSTM_layers = 2\n",
    "with tf.variable_scope('lstm'):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,\n",
    "                                            forget_bias=1.0)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers,\n",
    "                                      state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                       sequence_length=_seqlen,\n",
    "                                       dtype=tf.float32)\n",
    "\n",
    "# 需要修改 final_output:\n",
    "# Extract the final state and use in a linear layer \n",
    "final_output = tf.matmul(states[num_LSTM_layers][1],\n",
    "                        weights['linear_layer']) + biases['linear_layer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
