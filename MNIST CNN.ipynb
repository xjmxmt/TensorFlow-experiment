{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python3.6\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pooling_2X2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input, W)+b)\n",
    "\n",
    "def fully_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(100, 10)\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = x_train[:1000]\n",
    "x_test = x_test[:100]\n",
    "y_train = y_train[:1000]\n",
    "y_test = y_test[:100]\n",
    "\n",
    "# print(x_train.shape)\n",
    "\n",
    "# x_train = np.resize(x_train, [1000, 784])\n",
    "# x_test = np.resize(x_test, [100, 784])\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28, 28])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "x_image = tf.expand_dims(X, 3) # ?,28,28,1\n",
    "conv1 = conv_layer(x_image, [5,5,1,32])\n",
    "conv1_pool = max_pooling_2X2(conv1)\n",
    "\n",
    "conv2 = conv_layer(conv1_pool, [5,5,32,64])\n",
    "conv2_pool = max_pooling_2X2(conv2)\n",
    "\n",
    "conv2_flat = tf.reshape(conv2_pool, [-1,7*7*64])\n",
    "full_1 = tf.nn.relu(fully_layer(conv2_flat, 1024))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
    "\n",
    "full_2 = fully_layer(full1_drop, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-3c4f611430eb>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=full_2)\n",
    "gd = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(full_2, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.07000000029802322\n",
      "step 10, training accuracy 0.3400000035762787\n",
      "step 20, training accuracy 0.6100000143051147\n",
      "step 30, training accuracy 0.8899999856948853\n",
      "step 40, training accuracy 0.9700000286102295\n",
      "step 50, training accuracy 0.9599999785423279\n",
      "step 60, training accuracy 0.9900000095367432\n",
      "step 70, training accuracy 1.0\n",
      "step 80, training accuracy 1.0\n",
      "step 90, training accuracy 1.0\n",
      "test accuracy: 0.7300000190734863\n"
     ]
    }
   ],
   "source": [
    "STEPS = 100\n",
    "MINIBATCH_SIZE = 100\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "split_X = np.split(x_train, int(x_train.shape[0]/MINIBATCH_SIZE))\n",
    "split_y = np.split(y_train, int(y_train.shape[0]/MINIBATCH_SIZE))\n",
    "\n",
    "n_test = int(x_test.shape[0]/MINIBATCH_SIZE)\n",
    "split_X2 = np.split(x_test, int(x_test.shape[0]/MINIBATCH_SIZE))\n",
    "split_y2 = np.split(y_test, int(y_test.shape[0]/MINIBATCH_SIZE))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    \n",
    "    for i in range(STEPS):\n",
    "        k = 0\n",
    "        if i % 10 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={X:split_X[k], y:split_y[k], keep_prob:1.0})\n",
    "            print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "        sess.run(gd, feed_dict={X:split_X[k], y:split_y[k], keep_prob:0.5})\n",
    "        k += 1\n",
    "    test_accuracy = np.mean([sess.run(accuracy, feed_dict={X:split_X2[i], y:split_y2[i], keep_prob:1.0}) for i in range(n_test)])\n",
    "    print(\"test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
