{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "(50000, 1)\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "Tensor(\"strided_slice:0\", shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = np.transpose(x_train, (0,2,3,1))\n",
    "x_test = np.transpose(x_test, (0,2,3,1))\n",
    "# x_train = x_train[:10000]\n",
    "# x_test = x_test[:100]\n",
    "# y_train = y_train[:10000]\n",
    "# y_test = y_test[:100]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pooling_2X2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input, W)+b)\n",
    "\n",
    "def fully_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-71b4fe670156>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "conv_1 = conv_layer(X, [5,5,3,32])\n",
    "conv1_pool = max_pooling_2X2(conv_1)\n",
    "\n",
    "conv_2 = conv_layer(conv1_pool, [5,5,32,64])\n",
    "conv2_pool = max_pooling_2X2(conv_2)\n",
    "conv2_flat = tf.reshape(conv2_pool, [-1,8*8*64])\n",
    "\n",
    "full_1 = tf.nn.relu(fully_layer(conv2_flat, 1024))\n",
    "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
    "\n",
    "full_2 = fully_layer(full1_drop, 10)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=full_2)\n",
    "gd = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(full_2, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.10999999940395355\n",
      "test accuracy: 15.0%\n"
     ]
    }
   ],
   "source": [
    "STEPS = 10\n",
    "MINIBATCH_SIZE = 100\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "split_X = np.split(x_train, int(x_train.shape[0]/MINIBATCH_SIZE))\n",
    "split_y = np.split(y_train, int(y_train.shape[0]/MINIBATCH_SIZE))\n",
    "\n",
    "n_test = int(x_test.shape[0]/MINIBATCH_SIZE)\n",
    "split_X2 = np.split(x_test, int(x_test.shape[0]/MINIBATCH_SIZE))\n",
    "split_y2 = np.split(y_test, int(y_test.shape[0]/MINIBATCH_SIZE))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    \n",
    "    for i in range(STEPS):\n",
    "        k = 0\n",
    "        if i % 10 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={X:split_X[k], y:split_y[k], keep_prob:1.0})\n",
    "            print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "        sess.run(gd, feed_dict={X:split_X[k], y:split_y[k], keep_prob:0.5})\n",
    "        k += 1\n",
    "    test_accuracy = np.mean([sess.run(accuracy, feed_dict={X:split_X2[i], y:split_y2[i], keep_prob:1.0}) for i in range(n_test)])\n",
    "    print(\"test accuracy: {:.4}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12999999523162842\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.05000000074505806\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.1899999976158142\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.05000000074505806\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.12999999523162842\n",
      "step 0, training accuracy 0.05000000074505806\n",
      "step 0, training accuracy 0.05999999865889549\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.1599999964237213\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.05000000074505806\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.05999999865889549\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.05999999865889549\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.05999999865889549\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.1599999964237213\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.05999999865889549\n",
      "step 0, training accuracy 0.05000000074505806\n",
      "step 0, training accuracy 0.12999999523162842\n",
      "step 0, training accuracy 0.12999999523162842\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.1599999964237213\n",
      "step 0, training accuracy 0.03999999910593033\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.05999999865889549\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.12999999523162842\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.09000000357627869\n",
      "step 0, training accuracy 0.20999999344348907\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.10999999940395355\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.05999999865889549\n",
      "step 0, training accuracy 0.1599999964237213\n",
      "step 0, training accuracy 0.12999999523162842\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.1599999964237213\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 0, training accuracy 0.07999999821186066\n",
      "step 0, training accuracy 0.07000000029802322\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.11999999731779099\n",
      "step 0, training accuracy 0.14000000059604645\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.30000001192092896\n",
      "step 10, training accuracy 0.23999999463558197\n",
      "step 10, training accuracy 0.2199999988079071\n",
      "step 10, training accuracy 0.2199999988079071\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.17000000178813934\n",
      "step 10, training accuracy 0.20999999344348907\n",
      "step 10, training accuracy 0.20999999344348907\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.2199999988079071\n",
      "step 10, training accuracy 0.25999999046325684\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.20999999344348907\n",
      "step 10, training accuracy 0.1899999976158142\n",
      "step 10, training accuracy 0.27000001072883606\n",
      "step 10, training accuracy 0.2199999988079071\n",
      "step 10, training accuracy 0.25999999046325684\n",
      "step 10, training accuracy 0.1899999976158142\n",
      "step 10, training accuracy 0.1599999964237213\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.14000000059604645\n",
      "step 10, training accuracy 0.20999999344348907\n",
      "step 10, training accuracy 0.30000001192092896\n",
      "step 10, training accuracy 0.2199999988079071\n",
      "step 10, training accuracy 0.27000001072883606\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.25999999046325684\n",
      "step 10, training accuracy 0.17000000178813934\n",
      "step 10, training accuracy 0.23000000417232513\n",
      "step 10, training accuracy 0.1599999964237213\n",
      "step 10, training accuracy 0.18000000715255737\n",
      "step 10, training accuracy 0.25\n",
      "step 10, training accuracy 0.1899999976158142\n",
      "step 10, training accuracy 0.25\n",
      "step 10, training accuracy 0.20999999344348907\n",
      "step 10, training accuracy 0.23999999463558197\n",
      "step 10, training accuracy 0.20999999344348907\n",
      "step 10, training accuracy 0.25\n",
      "step 10, training accuracy 0.2199999988079071\n",
      "step 10, training accuracy 0.17000000178813934\n",
      "step 10, training accuracy 0.15000000596046448"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-dd476fe90af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"step {}, training accuracy {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_X2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msplit_y2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.6\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.6\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.6\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    390\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    391\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.6\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "\n",
    "STEPS = 100\n",
    "MINIBATCH_SIZE = 100\n",
    "\n",
    "split_X = np.split(x_train, int(x_train.shape[0]/MINIBATCH_SIZE))\n",
    "split_y = np.split(y_train, int(y_train.shape[0]/MINIBATCH_SIZE))\n",
    "\n",
    "n_test = int(x_test.shape[0]/MINIBATCH_SIZE)\n",
    "split_X2 = np.split(x_test, int(x_test.shape[0]/MINIBATCH_SIZE))\n",
    "split_y2 = np.split(y_test, int(y_test.shape[0]/MINIBATCH_SIZE))\n",
    "\n",
    "with g2.as_default():\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    C1, C2, C3 = 30, 50, 80 \n",
    "    F1 = 500\n",
    "    \n",
    "    conv1_1 = conv_layer(X, [3,3,3,C1])\n",
    "    conv1_2 = conv_layer(conv1_1, [3,3,C1,C1])\n",
    "    conv1_3 = conv_layer(conv1_2, [3,3,C1,C1])\n",
    "    conv1_pool = max_pooling_2X2(conv1_3)\n",
    "    conv1_drop = tf.nn.dropout(conv1_pool, keep_prob=keep_prob)\n",
    "\n",
    "    conv2_1 = conv_layer(conv1_drop, [3,3,C1,C2])\n",
    "    conv2_2 = conv_layer(conv2_1, [3,3,C2,C2])\n",
    "    conv2_3 = conv_layer(conv2_2, [3,3,C2,C2])\n",
    "    conv2_pool = max_pooling_2X2(conv2_3)\n",
    "    conv2_drop = tf.nn.dropout(conv2_pool, keep_prob=keep_prob)\n",
    "\n",
    "    conv3_1 = conv_layer(conv2_drop, shape=[3, 3, C2, C3]) \n",
    "    conv3_2 = conv_layer(conv3_1, shape=[3, 3, C3, C3]) \n",
    "    conv3_3 = conv_layer(conv3_2, shape=[3, 3, C3, C3]) \n",
    "    conv3_pool = tf.nn.max_pool(conv3_3, ksize=[1, 8, 8, 1], strides=[1, 8, 8, 1],padding='SAME') \n",
    "    conv3_flat = tf.reshape(conv3_pool, [-1, C3])\n",
    "\n",
    "    \n",
    "    conv3_drop = tf.nn.dropout(conv3_flat, keep_prob=keep_prob)\n",
    "    full1 = tf.nn.relu(fully_layer(conv3_flat, F1)) \n",
    "    full1_drop = tf.nn.dropout(full1, keep_prob=keep_prob)\n",
    "    full_2 = fully_layer(full1_drop, 10)\n",
    "\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=full_2)\n",
    "    gd = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(full_2, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)    \n",
    "        for i in range(STEPS):\n",
    "            for k in range(MINIBATCH_SIZE):\n",
    "                if i % 10 == 0:\n",
    "                    train_accuracy = sess.run(accuracy, feed_dict={X:split_X[k], y:split_y[k], keep_prob:1.0})\n",
    "                    print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "                sess.run(gd, feed_dict={X:split_X[k], y:split_y[k], keep_prob:0.5})\n",
    "        test_accuracy = np.mean([sess.run(accuracy, feed_dict={X:split_X2[i], y:split_y2[i], keep_prob:1.0}) for i in range(n_test)])\n",
    "        print(\"test accuracy: {:.4}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
